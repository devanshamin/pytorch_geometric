# Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text Attributed Graph Representation Learning

This repository implements the methodology introduced in the [paper](https://arxiv.org/abs/2305.19523) that leverages large language models (LLMs) to enhance text-attributed graph (TAG) representation learning, boosting graph neural network (GNN) performance on downstream tasks.

## Framework Overview

1. **Node Feature Extraction**

   - Prepare prompts containing the article information (title and abstract) for each node.
   - Query an LLM with these prompts to generate a ranked label prediction list and explanation.

1. **Node Feature Encoder**

   - Fine-tune a language model (LM) on a sequence classification task with the article title and abstract as input.

1. **GNN Trainer**

   - Train a GNN model using the following features, with node features updated by the fine-tuned LM encoder:
     1. Title & Abstract (TA)
     1. Prediction (P) - Using a PyTorch `nn.Embedding` layer for top-k ranked features.
     1. Explanation (E)

1. **Model Ensemble**

   - Fuse predictions from the trained GNN models on TA, P, and E by averaging them.

> \[!Note\]
> Fine-tuning an LM is optional and not currently supported. Instead, you can use any open-weight fine-tuned embedding model, significantly reducing time and cost while achieving comparable results.

## Usage

### Setup the environment

```bash
# Replace the 'cu118' CUDA version according to your system
pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu118
pip install torch_geometric
pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu118.html

# For online LLM inference
$ poetry install
# For offline LLM inference
$ poetry install --extras "llm_offline"
```

### Training

```bash
$ python train.py --config=train_config.yaml
# You can also provide CLI arguments to overwrite values in the `train_config.yaml` file
$ python train.py --help
```

- The [train_config.yaml](./train_config.yaml) utilizes the online LLM engine with the model [huggingface/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).
- Predictions generated by this model for the PubMed dataset have been uploaded to [Hugging Face](https://huggingface.co/datasets/devanshamin/PubMedDiabetes-LLM-Predictions), which will be downloaded and used instead of calling the LLM during training.
- This optimization significantly accelerates the training process and demonstrates end-to-end training with tape.
- Instead of fine-tuning an LM on the PubMed dataset, the [train_config.yaml](./train_config.yaml) uses a general-purpose embedding model [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0).
- With LLM predictions, you can expect the following run time and accuracy when training the GNN for the PubMed dataset using the feature type `TAPE`:

```markdown
When the LM embeddings cache for the dataset is empty,
Feature_type        Test_accuracy
TITLE_ABSTRACT (TA)      0.908722
PREDICTION (P)           0.889959
EXPLANATION (E)          0.914807
TAPE (TAPE)              0.946501
Run time: 11 minutes and 14.59 seconds

When the LM embeddings cache for the dataset is present,
Feature_type        Test_accuracy
TITLE_ABSTRACT (TA)      0.915061
PREDICTION (P)           0.889452
EXPLANATION (E)          0.923174
TAPE (TAPE)              0.952333
Run time: 1 minute and 0.31 seconds
```

In summary,

|                | Current Implementation                 | Author Implementation                   |
| -------------- | -------------------------------------- | --------------------------------------- |
| Dataset        | PubMed                                 | PubMed                                  |
| LLM            | `meta-llama/Meta-Llama-3-8B-Instruct`  | `openai/gpt-3.5-turbo-0301`             |
| LM fine-tuning | ✖                                      | ✔                                       |
| GNN layer      | `SAGEConv`                             | `SAGEConv`                              |
| GNN hparams    | `layers=4, hidden_dim=64, dropout=0.1` | `layers=3, hidden_dim=256, dropout=0.5` |
| Seed runs      | 4                                      | 4                                       |
| Accuracy       | `0.9573 ± 0.0032`                      | `0.9618 ± 0.0053`                       |
